{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import null_space\n",
    "from numpy.linalg import svd\n",
    "from scipy.optimize import linprog\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "def nullspace(A, atol=1e-13, rtol=0):\n",
    "    A = np.atleast_2d(A)\n",
    "    u, s, vh = svd(A)\n",
    "    tol = max(atol, rtol * s[0])\n",
    "    nnz = (s >= tol).sum()\n",
    "    ns = vh[nnz:].conj().T\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda')\n",
    "class admm(object):\n",
    "    def __init__(self, C, A, b, rho, alpha):\n",
    "        self.C = C\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        val = np.linalg.lstsq(self.A.numpy(), self.b.numpy())\n",
    "        self.x0 = torch.from_numpy(val[0])\n",
    "        self.Q = torch.from_numpy(null_space(self.A.numpy()))\n",
    "        self.x = self.x0\n",
    "        self.z = self.x\n",
    "        self.mu = torch.zeros(self.z.size(), dtype=torch.double)\n",
    "        self.omega = torch.zeros(self.z.size(), dtype=torch.double)\n",
    "        self.rho = rho\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def h(self, z):\n",
    "        if torch.min(z) < 0:\n",
    "            return math.inf\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def Pc(self, z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "    \n",
    "    def iterate(self):\n",
    "        val1 = torch.matmul(self.Q.t(), self.Q).inverse()\n",
    "        val2 = torch.matmul(self.Q.t(), self.z - self.x0 - self.mu)\n",
    "        val3 = 1/self.rho * torch.matmul(self.Q.t(), self.C)\n",
    "        self.omega = torch.matmul(val1, val2 - val3)\n",
    "        self.z = self.Pc(torch.matmul(self.Q, self.omega) + self.x0 + self.mu)\n",
    "        self.mu = self.mu + torch.matmul(self.Q, self.omega) - self.z + self.x0\n",
    "        \n",
    "    def currentVal(self):\n",
    "        val = torch.matmul(self.C.t(), (self.x0 + torch.matmul(self.Q, self.omega))) + self.h(self.z)\n",
    "        return val\n",
    "    \n",
    "    def loss(self):\n",
    "        val = linprog(self.C.view(self.C.shape[0]), A_eq = self.A, b_eq = self.b)\n",
    "        return abs(self.currentVal()[0][0].item() - val.fun)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/myenv/lib/python3.5/site-packages/ipykernel_launcher.py:7: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt = admm(C,A,b,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "blist = []\n",
    "for i in range(1000):\n",
    "    opt.iterate()\n",
    "    blist.append(opt.loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x108112fe10>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAEupJREFUeJzt3X+QZWdd5/H3J9MTWLJKEqfLCpnMTtBBHH+wxOZXQblZQXaSspKyZCkGd0GNzv4hii6rhnKLrPAX5RYqVSFhCmNWazcRhcJUzBIQw6ZqEZZJqTE/GBgByURwJiQgghom890/7rkzPZ177unpvj09z837VdV17z33yT3f02fqk6ef89zzpKqQJM2Xcza7AEnS7BnukjSHDHdJmkOGuyTNIcNdkuaQ4S5Jc8hwl6Q5ZLhL0hwy3CVpDi1s1o63bdtWO3fu3KzdS1KT7rnnnkeqanGo3aaF+86dOzlw4MBm7V6SmpTkb1bTzmEZSZpDhrskzSHDXZLmkOEuSXPIcJekOWS4S9IcMtwlaQ41F+4Hv/Q13vGhgzzyD/+82aVI0lmruXD/zJGv8c4/PcSjX398s0uRpLNWc+EeAoDrektSv/bCPZtdgSSd/ZoL97HCrrsk9Wku3Mcdd4dlJKlfe+HepbvhLkn9mgv3cd/dYRlJ6jcY7kluSnIkyX1T2lye5C+S3J/k/8y2xJX7Gj3ac5ekfqvpud8M7Ol7M8n5wLuAq6rqe4B/P5vSeva3kR8uSXNiMNyr6m7g0SlNXgu8v6q+0LU/MqPaJopzISVp0CzG3J8DXJDko0nuSfK6GXzmIIdlJKnfLNZQXQB+AHg58C+AP0vy8ar69MqGSfYB+wB27Nixpp2dmArpBVVJ6jWLnvth4M6q+npVPQLcDTxvUsOq2l9VS1W1tLg4uHj3RF5QlaRhswj3PwJelmQhyTOAFwEPzuBzJzoR7hu1A0maA4PDMkluAS4HtiU5DFwHbAWoqhur6sEkHwTuBY4D76mq3mmT63XyxmHGuyT1GQz3qtq7ija/Dvz6TCoa4mQZSRrU4DdUR+y3S1K/5sLdG4dJ0rD2wj0nJ0NKkiZrL9y7R3vuktSvvXB3KqQkDWov3F1DVZIGtRfuToWUpEHNhfuYX2KSpH7NhbtzZSRpWHPhjjcOk6RBzYV7XENVkga1F+6Oy0jSoPbCfbMLkKQGNBfuY3bcJalfc+E+vreMF1QlqV+D4T569IKqJPVrL9y7R3vuktSvvXD3xmGSNKi5cMc1VCVpUHPh7o3DJGlYc+E+Zr9dkvo1F+4nOu6muyT1Ggz3JDclOZLkvoF2L0hyLMmrZlfexP0AToWUpGlW03O/GdgzrUGSLcDbgQ/NoKapnAopScMGw72q7gYeHWj2c8D7gCOzKGqaeMtfSRq07jH3JBcDPwrcsP5yVrE/bx0mSYNmcUH1N4FfqarjQw2T7EtyIMmBo0ePrmundtwlqd/CDD5jCbi1u9C5DbgyybGq+sDKhlW1H9gPsLS0tKZ8PjksY7xLUp91h3tVXTp+nuRm4PZJwT5rRrsk9RsM9yS3AJcD25IcBq4DtgJU1Y0bWt3EekaPdtwlqd9guFfV3tV+WFX9xLqqWYWTF1RNd0nq0943VO25S9KgZsNdktSvuXAfs+MuSf2aC/fgGqqSNKS9cHcNVUka1F64d4/23CWpX3vh7hqqkjSouXDHG4dJ0qAGw33Ee8tIUr/mwt157pI0rL1w7x7tuEtSv/bC3TVUJWlQe+HePdpzl6R+7YW7Nw6TpEHthbtTISVpUHPhPmbHXZL6NRfurqEqScOaC/cxo12S+jUX7nGVPUka1GC4O89dkoa0F+6bXYAkNaC5cB/zeqok9RsM9yQ3JTmS5L6e9388yb1J/irJx5I8b/ZlLt/f6NFsl6R+q+m53wzsmfL+54B/U1XfB7wN2D+Dunq5hqokDVsYalBVdyfZOeX9jy17+XFg+/rL6ucaqpI0bNZj7tcA/3vGn3kKbxwmScMGe+6rleTfMgr3l01psw/YB7Bjx4417mj0YLZLUr+Z9NyTfD/wHuDqqvpyX7uq2l9VS1W1tLi4uLZ9ORlSkgatO9yT7ADeD/zHqvr0+ktaJcdlJKnX4LBMkluAy4FtSQ4D1wFbAarqRuAtwLcB7+q+PXqsqpY2qmCnQkrSsNXMltk78P5PAz89s4oGeEFVkoY19w3VE/eWMd0lqVd74d49Gu2S1K+9cHeyjCQNai7cxxyVkaR+zYX7iXvLbHIdknQ2ay7ccQ1VSRrUXLg75i5Jw9oL9+7Rjrsk9Wsv3F1DVZIGtRfum12AJDWguXAfc1hGkvo1F+7eOEyShrUX7q6hKkmD2gt311CVpEHNhfuYPXdJ6tdcuPslJkka1ly4S5KGNRfuJy+oOi4jSX3aC/cTNw7b3Dok6WzWXrh3j2a7JPVrL9zjPHdJGtJeuHePznOXpH6D4Z7kpiRHktzX836SvDPJoST3Jrls9mVKkk7HanruNwN7prx/BbCr+9kH3LD+svp5QVWShg2Ge1XdDTw6pcnVwO/WyMeB85NcNKsCVzp5P3dJUp9ZjLlfDDy07PXhbtuTJNmX5ECSA0ePHl3fXu26S1KvM3pBtar2V9VSVS0tLi6u+XMSe+6SNM0swv1h4JJlr7d32zZMsOMuSdPMItxvA17XzZp5MfDVqvriDD63V7x7mCRNtTDUIMktwOXAtiSHgeuArQBVdSNwB3AlcAj4BvCTG1Xscs5zl6R+g+FeVXsH3i/gZ2dW0So4LCNJ0zX3DVXwgqokDWkz3Ik9d0maoslwJ465S9I0TYZ7wHEZSZqiyXCXJE3XZLh7QVWSpmsz3IlrqErSFG2Ge5znLknTtBnuOCwjSdO0Ge5xnrskTdNmuG92AZJ0lmsy3MEvMUnSNG2GuxdUJWmqJsPdYRlJmq7NcI/z3CVpmkbD3amQkjRNm+G+2QVI0lmuyXAHL6hK0jRNhnsSp0JK0hRthjv23CVpmjbD3QuqkjTVqsI9yZ4kB5McSnLthPd3JLkryZ8nuTfJlbMv9ZQ92nOXpCkGwz3JFuB64ApgN7A3ye4Vzf4r8N6qej7wGuBdsy701JrAvrsk9VtNz/2FwKGq+mxVPQ7cCly9ok0B39o9fybwt7Mr8cmcCilJ0y2sos3FwEPLXh8GXrSizX8DPpTk54DzgFfMpLopHJaRpH6zuqC6F7i5qrYDVwK/l+RJn51kX5IDSQ4cPXp0zTtzJSZJmm414f4wcMmy19u7bctdA7wXoKr+DHg6sG3lB1XV/qpaqqqlxcXFtVVMt4aqY+6S1Gs14f5JYFeSS5Ocy+iC6W0r2nwBeDlAku9mFO5r75oPsOcuSdMNhntVHQPeANwJPMhoVsz9Sd6a5Kqu2ZuAn0nyl8AtwE/UBt620TVUJWm61VxQparuAO5Yse0ty54/ALx0tqX1S5wvI0nTNPkNVXBYRpKmaTfcHZiRpF5NhnscdJekqZoNd7Ndkvq1Ge64hqokTdNmuNtzl6Sp2gz3zS5Aks5yTYY7OBVSkqZpMtxHa6hKkvq0Ge7gBVVJmqLJcMcLqpI0VZPh7ip7kjRdm+HujcMkaaomwx28t4wkTdNkuI8uqG52FZJ09moz3F2JSZKmajPcXUNVkqZqM9ztuUvSVE2GOzgTUpKmaTLcnQopSdM1Ge7gsIwkTdNkuI/67aa7JPVZVbgn2ZPkYJJDSa7tafPqJA8kuT/J/5ptmSv3Zc9dkqYZDPckW4DrgSuA3cDeJLtXtNkFvBl4aVV9D/ALG1Drsv3Bw1/5R/7Hxz7PP33ziY3clSQ1aTU99xcCh6rqs1X1OHArcPWKNj8DXF9VjwFU1ZHZlnmqED71pa9x3W33c9enNnRXktSk1YT7xcBDy14f7rYt9xzgOUn+b5KPJ9kzqwInWT5Z5guPfmMjdyVJTVqY4efsAi4HtgN3J/m+qvrK8kZJ9gH7AHbs2DGTHR9+7B9n8jmSNE9W03N/GLhk2evt3bblDgO3VdU3q+pzwKcZhf0pqmp/VS1V1dLi4uJaaz5lgezDj32DR7/+OMeeOL7mz5OkebOanvsngV1JLmUU6q8BXruizQeAvcDvJNnGaJjms7Ms9BTLxmXuOniUy972YbZuCVvOCcePwxNOpZF0FvtPP/hsfnnPczd0H4PhXlXHkrwBuBPYAtxUVfcneStwoKpu6957ZZIHgCeAX6qqL29U0Su/n/r925/JS79zG08cL85J2HLO6KKrJJ2NXnDphRu+j1WNuVfVHcAdK7a9ZdnzAv5z97Phxh33F116IT/2A9u56nnP4ulbt5yJXUtSE2Z1QfWMGvfJz3/GVl69dMnUtpL0VNTm7Qe6rvvWLU2WL0kbrul0PNdwl6SJmkzH8bDMwhYvmkrSJG2Ge5fpDstI0mRNpuN4mqPhLkmTtZmOJ3ruDstI0iRNhvvJMfcmy5ekDdd0OjosI0mTNZmOTxwf3TvmXIdlJGmiJsP9WBfuDstI0mRNpuOx46Pb+zosI0mTNZmOx54Y9dydLSNJk7UZ7sfH4d5k+ZK04ZpMx/GqSwvn2HOXpEmaDPfxSkvnLjRZviRtuCbT0dsPSNJ0TaejwzKSNFmT4X7irpAOy0jSRE2n49Zzmi5fkjZM0+noPHdJmqzJcPeukJI03arSMcmeJAeTHEpy7ZR2P5akkizNrsR+rqEqSZMNpmOSLcD1wBXAbmBvkt0T2n0L8EbgE7MucsK+ANi64LCMJE2ymq7vC4FDVfXZqnocuBW4ekK7twFvB/5phvVNdGJYxqmQkjTRasL9YuChZa8Pd9tOSHIZcElV/fEMa+uXJz2RJC2z7kHrJOcA7wDetIq2+5IcSHLg6NGj6901UDP4DEmaP6sJ94eBS5a93t5tG/sW4HuBjyb5PPBi4LZJF1Wran9VLVXV0uLi4pqLPu/che6ZPXdJmmRhuAmfBHYluZRRqL8GeO34zar6KrBt/DrJR4H/UlUHZlvqSTf8h8t43z0P8x2L523ULiSpaYM996o6BrwBuBN4EHhvVd2f5K1JrtroAifZfsEzeOMrdp2YNSNJOtVqeu5U1R3AHSu2vaWn7eXrL0uStB5+C0iS5pDhLklzyHCXpDlkuEvSHDLcJWkOGe6SNIcMd0maQ6nanPuzJDkK/M0a//NtwCMzLKcFHvNTg8f81LCeY/5XVTV4/5ZNC/f1SHKgqs7IgiBnC4/5qcFjfmo4E8fssIwkzSHDXZLmUKvhvn+zC9gEHvNTg8f81LDhx9zkmLskabpWe+6SpCmaC/cke5IcTHIoybWbXc+sJLkkyV1JHkhyf5I3dtsvTPLhJJ/pHi/otifJO7vfw73dOrbNSbIlyZ8nub17fWmST3TH9ftJzu22P617fah7f+dm1r0eSc5P8odJPpXkwSQvmefznOQXu3/T9yW5JcnT5/E8J7kpyZEk9y3bdtrnNcnru/afSfL6tdbTVLgn2QJcD1wB7Ab2Jtm9uVXNzDHgTVW1m9FShT/bHdu1wEeqahfwke41jH4Hu7qffcANZ77kmXgjo0Vgxt4O/EZVfSfwGHBNt/0a4LFu+2907Vr1W8AHq+q5wPMYHf9cnuckFwM/DyxV1fcCWxit5jaP5/lmYM+Kbad1XpNcCFwHvAh4IXDd+H8Ip62qmvkBXgLcuez1m4E3b3ZdG3SsfwT8MHAQuKjbdhFwsHv+bmDvsvYn2rXyw2g93o8APwTczmhR3EeAhZXnm9FKYC/pni907bLZx7CGY34m8LmVtc/reQYuBh4CLuzO2+3Av5vX8wzsBO5b63kF9gLvXrb9lHan89NUz52T/1DGDnfb5kr3p+jzgU8A315VX+ze+hLw7d3zefhd/Cbwy8Dx7vW3AV+p0dKOcOoxnTje7v2vdu1bcylwFPidbjjqPUnOY07Pc1U9DPx34AvAFxmdt3uY//M8drrndWbnu7Vwn3tJ/iXwPuAXqurvl79Xo/+Vz8X0piQ/Ahypqns2u5YzbAG4DLihqp4PfJ2Tf6oDc3eeLwCuZvQ/tWcB5/HkoYunhDN9XlsL94eBS5a93t5tmwtJtjIK9v9ZVe/vNv9dkou69y8CjnTbW/9dvBS4KsnngVsZDc38FnB+kvHavsuP6cTxdu8/E/jymSx4Rg4Dh6vqE93rP2QU9vN6nl8BfK6qjlbVN4H3Mzr3836ex073vM7sfLcW7p8EdnVX2s9ldGHmtk2uaSaSBPht4MGqeseyt24DxlfMX89oLH68/XXdVfcXA19d9uffWa+q3lxV26tqJ6Pz+KdV9ePAXcCrumYrj3f8e3hV17653m1VfQl4KMl3dZteDjzAnJ5nRsMxL07yjO7f+Ph45/o8L3O65/VO4JVJLuj+6nllt+30bfYFiDVcsLgS+DTw18CvbnY9MzyulzH6k+1e4C+6nysZjTd+BPgM8CfAhV37MJo59NfAXzGajbDpx7HGY78cuL17/mzg/wGHgD8AntZtf3r3+lD3/rM3u+51HO+/Bg505/oDwAXzfJ6BXwM+BdwH/B7wtHk8z8AtjK4rfJPRX2jXrOW8Aj/VHf8h4CfXWo/fUJWkOdTasIwkaRUMd0maQ4a7JM0hw12S5pDhLklzyHCXpDlkuEvSHDLcJWkO/X+BMimvF/zCKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(blist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "def noise(matrix, error):\n",
    "    return matrix + error * torch.rand(A.shape, dtype=torch.double)\n",
    "\n",
    "print(A.shape)\n",
    "print(b.shape)\n",
    "def findAnswer(c, A, b):\n",
    "    val = linprog(c.view(c.shape[0]), A_eq = A, b_eq = b)\n",
    "    \n",
    "    return val.fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, c, A, b):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.c = c\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(torch.DoubleTensor(x))\n",
    "        out = self.layer1(x.double())\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        self.opt = admm(self.c, self.A, self.b, out, 1)\n",
    "        for i in range(100):\n",
    "            opt.iterate()\n",
    "        return opt.currentVal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customloss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(customloss ,self).__init__()\n",
    "        \n",
    "    def forward(self,output, c, A, b):\n",
    "        val = linprog(c.view(c.shape[0]), A_eq = A, b_eq = b)\n",
    "        return abs(output - val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "N = 100\n",
    "A = torch.rand(M, N, dtype=torch.double)\n",
    "c = torch.rand(N, 1, dtype=torch.double)\n",
    "b = torch.randn(M, 1, dtype=torch.double)\n",
    "\n",
    "model = ConvNet(c, A, b)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[7.8764e-01, 9.2092e-01, 8.1775e-01, 5.6878e-01, 7.6794e-02,\n",
      "           6.6136e-01, 1.9277e-01, 6.7316e-01, 7.8083e-01, 2.3341e-01,\n",
      "           1.7708e-01, 8.6890e-01, 9.1595e-01, 5.1701e-01, 7.7243e-03,\n",
      "           6.0783e-01, 7.3598e-01, 2.7621e-01, 8.0751e-01, 4.4521e-01,\n",
      "           3.5092e-01, 9.4563e-01, 6.9037e-01, 9.2635e-01, 5.8150e-01,\n",
      "           3.0151e-01, 6.9074e-01, 1.9030e-01, 4.3456e-01, 9.1743e-01,\n",
      "           8.1507e-01, 9.4023e-01, 1.9603e-01, 2.7429e-01, 9.5988e-02,\n",
      "           7.9811e-01, 3.6703e-01, 5.5298e-02, 6.5794e-01, 7.4088e-01,\n",
      "           8.3140e-01, 7.8769e-01, 4.8419e-01, 2.9510e-01, 9.2545e-01,\n",
      "           7.9713e-02, 7.8938e-02, 9.5949e-01, 3.1167e-01, 4.8417e-01,\n",
      "           8.5809e-01, 7.8849e-01, 5.4964e-01, 8.9678e-01, 9.5012e-01,\n",
      "           5.4069e-01, 8.0751e-02, 4.9655e-01, 8.3880e-01, 5.5966e-01,\n",
      "           9.4222e-01, 2.5794e-01, 4.4086e-01, 1.3759e-01, 6.2835e-01,\n",
      "           2.6664e-01, 5.1090e-01, 3.8254e-01, 6.5602e-01, 2.3395e-01,\n",
      "           3.2051e-01, 4.7775e-01, 8.5280e-01, 2.6672e-01, 4.9644e-01,\n",
      "           3.5964e-01, 5.8241e-01, 5.4709e-01, 5.7767e-01, 7.5898e-01,\n",
      "           6.0601e-01, 7.3299e-02, 7.9790e-01, 3.5459e-01, 7.2751e-01,\n",
      "           5.4970e-01, 4.7314e-02, 6.8470e-03, 3.6155e-02, 7.9666e-01,\n",
      "           4.6655e-01, 6.2431e-01, 5.3357e-01, 8.6276e-03, 1.9780e-01,\n",
      "           7.3234e-01, 6.0570e-02, 5.6315e-01, 7.0451e-01, 4.2585e-01],\n",
      "          [7.0273e-01, 4.8831e-01, 1.7395e-01, 9.4949e-01, 6.2018e-01,\n",
      "           9.4079e-01, 1.4702e-01, 1.9905e-01, 3.7177e-01, 9.7518e-01,\n",
      "           8.4499e-01, 8.6661e-01, 2.8951e-01, 4.0213e-01, 2.0576e-01,\n",
      "           4.7012e-01, 7.0122e-01, 5.8496e-01, 8.3127e-01, 8.3115e-01,\n",
      "           2.6460e-02, 6.0427e-01, 4.6629e-01, 6.8574e-01, 5.5650e-02,\n",
      "           9.3314e-01, 4.0043e-01, 7.6001e-01, 5.3612e-01, 3.0681e-01,\n",
      "           3.8077e-01, 7.8673e-01, 6.5353e-02, 8.2166e-01, 6.6504e-01,\n",
      "           2.9931e-01, 5.1794e-02, 6.4034e-01, 2.7733e-01, 1.7488e-01,\n",
      "           6.6162e-01, 2.2557e-01, 8.5774e-01, 3.6832e-01, 7.5832e-01,\n",
      "           5.9546e-01, 3.3783e-02, 6.4114e-01, 8.5529e-01, 1.5420e-01,\n",
      "           1.1623e-01, 7.1910e-01, 2.9901e-01, 8.1732e-01, 7.4048e-01,\n",
      "           9.8887e-01, 9.1213e-01, 5.3399e-01, 1.8720e-01, 8.7901e-03,\n",
      "           3.3389e-01, 6.6117e-01, 7.7630e-01, 5.3531e-01, 1.2450e-01,\n",
      "           7.5911e-01, 8.4861e-01, 5.4495e-01, 2.1884e-01, 7.8064e-01,\n",
      "           1.1985e-01, 1.8474e-01, 6.9643e-01, 8.9988e-01, 4.5952e-01,\n",
      "           9.2249e-01, 8.0310e-01, 4.6086e-01, 1.5012e-01, 9.1925e-01,\n",
      "           6.7216e-01, 5.7992e-01, 3.6820e-01, 6.7160e-01, 7.3779e-01,\n",
      "           9.8376e-01, 9.6951e-01, 4.8010e-01, 3.3156e-01, 4.3548e-01,\n",
      "           6.5357e-01, 6.0262e-01, 8.8482e-01, 4.2836e-01, 4.9383e-01,\n",
      "           7.7322e-01, 8.9416e-01, 5.6875e-01, 8.6477e-01, 5.3565e-01],\n",
      "          [1.6849e-01, 5.5237e-02, 7.6976e-01, 3.2004e-01, 7.2243e-01,\n",
      "           3.3753e-01, 6.4641e-01, 6.5491e-01, 6.0691e-01, 7.3063e-01,\n",
      "           8.4168e-01, 1.7412e-01, 3.1494e-01, 8.2663e-01, 6.0608e-01,\n",
      "           6.6281e-01, 9.4119e-01, 1.0136e-01, 6.3302e-01, 5.5032e-01,\n",
      "           6.0367e-01, 1.1266e-01, 1.0928e-01, 8.3629e-01, 5.4623e-02,\n",
      "           6.8723e-01, 2.4967e-01, 3.7443e-01, 5.8527e-01, 8.3008e-01,\n",
      "           9.8508e-01, 7.2201e-01, 9.0785e-01, 7.3195e-01, 5.7369e-01,\n",
      "           8.1514e-01, 1.3072e-01, 8.5258e-01, 5.7857e-01, 4.2161e-01,\n",
      "           9.1367e-01, 4.8200e-01, 1.4811e-01, 1.4212e-01, 3.5617e-01,\n",
      "           2.0370e-01, 9.1248e-01, 7.3642e-01, 7.5646e-01, 5.6341e-01,\n",
      "           6.0641e-01, 4.6675e-01, 9.5457e-01, 1.9641e-01, 9.9241e-01,\n",
      "           8.4100e-01, 7.4618e-01, 7.5299e-01, 6.7908e-01, 4.7503e-01,\n",
      "           2.4851e-02, 7.2458e-01, 9.6792e-01, 8.0646e-01, 6.6063e-01,\n",
      "           5.7004e-02, 2.9380e-02, 4.3727e-01, 4.5956e-02, 1.8809e-02,\n",
      "           1.5459e-01, 1.8536e-01, 3.0170e-01, 8.6834e-01, 1.7801e-01,\n",
      "           4.7069e-01, 2.8210e-01, 5.6550e-01, 5.5131e-01, 5.8504e-01,\n",
      "           1.2274e-01, 5.7835e-01, 2.3104e-01, 3.0318e-01, 1.3163e-01,\n",
      "           6.3321e-01, 5.7002e-01, 8.2046e-01, 2.8755e-01, 2.8817e-01,\n",
      "           3.2605e-01, 2.0826e-01, 4.3657e-01, 5.8558e-01, 3.4722e-01,\n",
      "           2.6693e-01, 5.4367e-01, 2.6865e-01, 5.2787e-01, 3.8495e-01],\n",
      "          [4.3376e-01, 5.6626e-01, 3.1031e-01, 4.6432e-01, 7.0899e-01,\n",
      "           3.4667e-01, 8.3965e-01, 4.7505e-01, 8.5172e-01, 6.4868e-01,\n",
      "           8.4119e-01, 2.0747e-01, 9.9925e-01, 4.7878e-01, 8.1773e-02,\n",
      "           2.8587e-01, 7.7429e-01, 2.6434e-01, 4.5739e-02, 4.9971e-01,\n",
      "           6.6217e-02, 6.2885e-01, 3.6646e-01, 2.8510e-01, 7.7639e-01,\n",
      "           3.7366e-02, 5.0133e-01, 9.9789e-01, 6.2377e-01, 2.3549e-01,\n",
      "           3.8860e-01, 9.4703e-01, 9.9078e-01, 6.5091e-01, 1.4404e-01,\n",
      "           5.5629e-01, 5.0136e-01, 2.5911e-01, 5.3924e-01, 4.7260e-01,\n",
      "           9.4424e-01, 1.7776e-01, 7.0900e-01, 7.1602e-01, 8.6054e-01,\n",
      "           9.6636e-01, 2.6324e-01, 1.5184e-01, 5.4996e-01, 4.0175e-01,\n",
      "           9.4847e-01, 5.9886e-01, 7.9088e-01, 6.9242e-01, 7.3131e-01,\n",
      "           2.1885e-01, 2.2720e-01, 8.6847e-01, 5.7242e-01, 9.7483e-01,\n",
      "           1.6431e-01, 2.5724e-01, 7.9713e-01, 8.7577e-01, 1.8032e-01,\n",
      "           6.0352e-01, 2.3187e-01, 1.3859e-01, 6.3443e-01, 4.5003e-02,\n",
      "           5.5236e-01, 2.4520e-01, 1.4972e-01, 8.7515e-01, 7.5872e-01,\n",
      "           8.0877e-01, 4.2874e-01, 7.8242e-02, 3.9825e-01, 3.8337e-01,\n",
      "           2.3658e-02, 8.2525e-01, 8.3939e-01, 6.2819e-01, 9.7720e-01,\n",
      "           9.8836e-01, 9.6469e-01, 8.6477e-01, 7.6113e-01, 1.9300e-02,\n",
      "           7.1064e-02, 5.1337e-01, 5.7877e-03, 1.1492e-01, 5.2134e-01,\n",
      "           3.0244e-01, 7.2756e-01, 1.0955e-01, 1.0724e-02, 2.5052e-01],\n",
      "          [1.2267e-01, 7.8077e-01, 6.6528e-01, 4.1977e-01, 8.9575e-01,\n",
      "           3.8271e-01, 3.7268e-01, 9.7528e-01, 9.7756e-01, 4.1249e-02,\n",
      "           4.1227e-01, 2.4299e-01, 6.4045e-01, 2.9411e-01, 2.4349e-01,\n",
      "           5.3652e-01, 1.8661e-01, 8.1070e-01, 9.6911e-01, 9.5325e-01,\n",
      "           7.1129e-01, 7.6158e-01, 3.5901e-01, 4.3521e-01, 9.2969e-01,\n",
      "           2.4157e-01, 8.3194e-01, 7.6751e-01, 4.5318e-01, 5.7214e-01,\n",
      "           5.4976e-01, 3.8800e-02, 3.6971e-01, 1.6266e-01, 5.5885e-01,\n",
      "           4.3325e-01, 2.5992e-01, 9.6096e-02, 3.9591e-01, 5.1008e-01,\n",
      "           6.7835e-01, 6.2385e-02, 9.9695e-01, 9.0753e-01, 7.3074e-01,\n",
      "           3.1226e-01, 8.6818e-01, 8.1803e-01, 3.1210e-01, 8.9685e-01,\n",
      "           4.4205e-01, 5.6853e-01, 3.4536e-01, 4.1053e-01, 1.0508e-01,\n",
      "           5.7901e-01, 3.7527e-01, 6.9326e-02, 2.6135e-02, 2.5576e-02,\n",
      "           2.2063e-01, 4.6484e-01, 7.4994e-01, 4.6378e-02, 7.2100e-02,\n",
      "           5.3389e-01, 8.2517e-01, 6.6074e-01, 8.2726e-01, 7.9782e-02,\n",
      "           7.6530e-01, 4.5152e-01, 8.0344e-02, 9.9532e-01, 8.5838e-01,\n",
      "           1.3064e-01, 6.0097e-02, 7.1572e-01, 4.9690e-01, 2.6483e-01,\n",
      "           8.1117e-01, 6.1533e-01, 6.6047e-01, 5.1796e-01, 9.2914e-02,\n",
      "           3.7818e-01, 1.5628e-02, 9.2019e-01, 3.2020e-01, 9.8950e-01,\n",
      "           2.1191e-01, 8.9963e-01, 1.2306e-03, 9.7264e-01, 9.5234e-01,\n",
      "           3.5381e-01, 3.2495e-02, 6.6288e-01, 6.4528e-01, 4.6944e-01],\n",
      "          [8.3159e-01, 7.8492e-01, 1.6681e-01, 4.3843e-01, 2.4701e-01,\n",
      "           7.3115e-01, 2.1355e-01, 6.9954e-01, 7.1580e-01, 4.2657e-01,\n",
      "           1.0509e-01, 5.6428e-01, 4.3611e-01, 9.0949e-01, 8.4031e-01,\n",
      "           3.6536e-01, 3.2174e-01, 6.5609e-01, 6.4061e-01, 5.6032e-01,\n",
      "           8.0697e-01, 2.3560e-01, 8.2390e-01, 8.4182e-02, 3.6948e-01,\n",
      "           9.5909e-01, 9.2712e-01, 7.2856e-01, 1.9208e-01, 3.6766e-01,\n",
      "           1.0163e-01, 9.1162e-01, 4.5428e-02, 4.5737e-01, 4.3610e-01,\n",
      "           8.6252e-01, 4.3286e-01, 6.9585e-01, 1.5231e-01, 1.5027e-01,\n",
      "           5.7639e-01, 8.3483e-01, 7.6384e-01, 4.7472e-01, 8.7661e-01,\n",
      "           5.9812e-01, 7.4455e-01, 3.5197e-01, 6.8714e-01, 3.0718e-01,\n",
      "           7.7995e-02, 5.1877e-01, 3.6561e-01, 2.7556e-01, 4.7177e-02,\n",
      "           9.8134e-01, 1.6690e-01, 6.3570e-01, 8.8242e-02, 1.3648e-02,\n",
      "           6.6575e-01, 9.0518e-01, 6.3170e-03, 4.3796e-01, 5.3115e-01,\n",
      "           8.1189e-01, 1.8611e-01, 2.8614e-01, 1.2475e-01, 1.7680e-01,\n",
      "           8.0217e-01, 4.5110e-01, 2.3656e-01, 2.4590e-01, 1.0757e-01,\n",
      "           2.6673e-01, 1.7329e-01, 5.6437e-01, 3.0479e-01, 9.5563e-01,\n",
      "           7.6763e-02, 5.7597e-01, 4.1265e-01, 2.1245e-01, 4.7714e-01,\n",
      "           2.8450e-01, 9.1796e-01, 5.7812e-01, 6.4656e-01, 2.5251e-01,\n",
      "           9.3214e-01, 8.4905e-01, 4.7072e-01, 2.1544e-01, 2.1092e-01,\n",
      "           4.2402e-01, 9.9594e-01, 8.1619e-01, 1.3361e-01, 5.1521e-01],\n",
      "          [5.1018e-01, 8.9944e-01, 8.6049e-01, 4.0892e-01, 1.7791e-01,\n",
      "           9.4184e-02, 5.8590e-01, 4.5582e-01, 6.2710e-01, 8.8116e-01,\n",
      "           2.8778e-01, 9.8763e-01, 8.7637e-01, 9.9764e-01, 9.7752e-01,\n",
      "           3.7310e-01, 6.2005e-01, 7.0314e-02, 5.6388e-01, 5.8280e-01,\n",
      "           6.9023e-01, 4.1767e-01, 1.0347e-01, 2.5775e-01, 4.9370e-01,\n",
      "           1.4941e-02, 8.6288e-01, 7.1919e-01, 1.2296e-01, 6.5887e-01,\n",
      "           2.9388e-01, 7.8362e-01, 2.1507e-01, 6.7662e-01, 3.2129e-01,\n",
      "           3.6018e-02, 6.4900e-01, 2.7060e-01, 6.8714e-01, 3.0609e-01,\n",
      "           1.0382e-01, 9.1827e-01, 1.1286e-01, 4.1059e-02, 7.0610e-01,\n",
      "           1.6263e-01, 3.3009e-01, 1.8991e-01, 4.8934e-01, 5.4033e-01,\n",
      "           6.7064e-01, 4.8388e-01, 8.3764e-01, 5.2933e-01, 1.9012e-01,\n",
      "           6.7214e-01, 7.8035e-01, 3.0092e-01, 9.4700e-01, 1.0029e-01,\n",
      "           8.5396e-01, 5.1400e-01, 1.4486e-01, 9.5316e-01, 7.7546e-01,\n",
      "           5.4481e-01, 7.3045e-01, 4.0393e-01, 1.7733e-01, 8.4598e-01,\n",
      "           1.3863e-01, 5.2090e-01, 7.2210e-01, 2.3884e-01, 1.7591e-01,\n",
      "           2.9948e-01, 9.5752e-01, 9.7687e-01, 7.1675e-01, 2.1167e-02,\n",
      "           4.6226e-01, 7.5365e-01, 6.9415e-01, 6.0237e-01, 2.9911e-01,\n",
      "           5.1578e-01, 4.8249e-01, 2.3989e-01, 3.7424e-01, 2.6429e-01,\n",
      "           2.4057e-01, 9.8732e-01, 4.8702e-01, 5.3637e-01, 9.7457e-02,\n",
      "           9.4405e-01, 9.6875e-01, 9.7178e-01, 7.4392e-01, 9.6473e-01],\n",
      "          [9.3065e-01, 3.8714e-01, 1.8645e-01, 6.7226e-01, 3.0863e-01,\n",
      "           9.7974e-01, 2.2035e-01, 6.1738e-01, 9.9755e-01, 2.2254e-01,\n",
      "           8.1641e-01, 2.8435e-01, 6.9482e-01, 4.5323e-01, 1.3415e-01,\n",
      "           8.8947e-01, 3.9568e-01, 9.3725e-01, 6.0389e-02, 2.0575e-01,\n",
      "           3.3177e-01, 6.4529e-01, 3.9631e-01, 4.3170e-01, 3.2665e-01,\n",
      "           6.9108e-01, 1.9508e-01, 2.1379e-01, 8.2189e-02, 1.8828e-01,\n",
      "           1.5930e-02, 5.3459e-01, 8.9533e-01, 7.2958e-01, 3.5945e-01,\n",
      "           4.3662e-01, 5.5299e-01, 2.3103e-01, 3.9154e-03, 5.6107e-01,\n",
      "           8.5849e-01, 3.3614e-01, 3.4418e-02, 4.9635e-01, 5.3331e-01,\n",
      "           2.1103e-01, 5.2023e-01, 4.2339e-01, 5.2438e-01, 2.4039e-02,\n",
      "           2.7507e-01, 2.4517e-02, 4.6535e-01, 8.7875e-01, 7.2243e-01,\n",
      "           6.9135e-01, 4.3923e-01, 3.9802e-01, 1.4396e-01, 5.7794e-01,\n",
      "           5.7713e-01, 8.1011e-01, 4.3352e-01, 2.5013e-01, 6.9712e-01,\n",
      "           3.0530e-01, 4.8493e-01, 6.7056e-01, 5.0452e-01, 9.8473e-01,\n",
      "           1.5798e-01, 1.4250e-01, 5.4983e-01, 3.8229e-01, 1.8693e-01,\n",
      "           1.0548e-01, 7.5801e-01, 8.4172e-01, 2.7557e-01, 1.5217e-01,\n",
      "           4.8588e-01, 6.7421e-01, 9.9961e-01, 7.9803e-01, 1.4590e-01,\n",
      "           7.0897e-01, 1.3036e-01, 8.2742e-01, 4.8579e-01, 9.2172e-01,\n",
      "           6.6659e-01, 7.0612e-02, 3.3856e-01, 3.1572e-01, 2.6870e-01,\n",
      "           4.8297e-01, 2.9539e-01, 5.0920e-01, 7.1754e-01, 4.3565e-01],\n",
      "          [5.0860e-01, 6.9927e-01, 1.9633e-01, 7.5962e-01, 8.5370e-01,\n",
      "           1.1646e-01, 2.2860e-01, 8.2967e-02, 8.0097e-01, 8.4093e-01,\n",
      "           2.3802e-01, 6.9623e-02, 2.7147e-01, 8.0040e-01, 6.0614e-01,\n",
      "           9.6508e-01, 8.7439e-01, 2.0629e-04, 8.7965e-01, 9.4940e-01,\n",
      "           1.2116e-01, 3.0340e-01, 9.1102e-01, 8.0904e-01, 5.9783e-02,\n",
      "           1.6533e-01, 8.8094e-01, 2.6642e-01, 1.5434e-01, 7.0453e-01,\n",
      "           9.5583e-01, 5.5758e-01, 6.8444e-01, 1.3151e-01, 7.2303e-01,\n",
      "           3.3700e-01, 6.0595e-01, 4.4017e-01, 6.4984e-01, 6.9581e-01,\n",
      "           4.6496e-01, 2.8952e-01, 1.2016e-01, 1.6442e-01, 9.1501e-01,\n",
      "           1.7870e-02, 9.2589e-01, 2.4807e-01, 3.1573e-01, 1.4930e-02,\n",
      "           1.0203e-01, 4.8633e-01, 2.4843e-01, 1.2159e-01, 5.2364e-01,\n",
      "           1.4636e-01, 1.7375e-01, 6.6491e-01, 5.3665e-01, 2.7096e-01,\n",
      "           1.8098e-01, 2.4143e-02, 5.6016e-01, 4.8088e-01, 6.8112e-01,\n",
      "           9.6947e-01, 5.8036e-01, 2.3846e-01, 9.8971e-01, 3.4213e-01,\n",
      "           5.9771e-01, 8.0399e-01, 3.4599e-01, 5.2571e-01, 7.6642e-01,\n",
      "           8.0042e-01, 9.6607e-01, 7.9636e-01, 9.9441e-01, 7.5016e-01,\n",
      "           4.0467e-02, 2.4598e-02, 6.3406e-01, 9.0188e-01, 5.8984e-01,\n",
      "           2.3156e-01, 1.7117e-01, 3.5353e-01, 5.1973e-01, 1.0786e-01,\n",
      "           9.6745e-01, 7.5198e-01, 3.9464e-01, 5.6406e-02, 1.7370e-02,\n",
      "           5.8979e-01, 8.6365e-01, 4.9386e-01, 4.2458e-01, 3.4563e-01],\n",
      "          [9.2620e-01, 8.6666e-01, 3.7583e-01, 1.8160e-01, 1.6410e-01,\n",
      "           1.9451e-01, 8.3998e-01, 3.7942e-01, 3.8712e-01, 5.0611e-01,\n",
      "           5.6925e-01, 8.6003e-01, 3.1108e-01, 4.6856e-01, 8.1040e-01,\n",
      "           8.1895e-01, 8.8749e-01, 3.4876e-01, 1.9870e-02, 7.7628e-02,\n",
      "           4.6996e-01, 1.7937e-01, 1.0759e-01, 9.7769e-01, 8.3418e-01,\n",
      "           8.4498e-01, 8.1105e-01, 2.1363e-01, 9.1849e-01, 5.1411e-01,\n",
      "           2.3447e-01, 5.9035e-01, 9.0095e-01, 8.4860e-01, 8.9646e-01,\n",
      "           4.6833e-01, 5.7313e-01, 8.8019e-01, 1.8579e-01, 3.4794e-01,\n",
      "           4.3950e-01, 8.6629e-01, 3.5031e-01, 1.6687e-01, 2.0825e-01,\n",
      "           8.9091e-02, 4.3967e-02, 8.1971e-01, 6.3247e-01, 9.5964e-02,\n",
      "           2.5071e-01, 7.7034e-01, 1.2169e-01, 1.5014e-01, 3.3127e-01,\n",
      "           7.5155e-02, 4.0692e-01, 7.1494e-01, 3.1447e-01, 5.0316e-01,\n",
      "           7.8065e-01, 6.9575e-01, 6.1995e-01, 7.7981e-01, 2.0398e-01,\n",
      "           5.0667e-01, 7.0460e-01, 7.2966e-01, 8.8556e-01, 8.7874e-01,\n",
      "           9.3462e-01, 7.3533e-01, 3.8651e-01, 5.3566e-01, 3.6693e-01,\n",
      "           5.3491e-01, 2.9528e-03, 5.6021e-01, 2.1487e-02, 9.1515e-01,\n",
      "           8.1776e-01, 6.2976e-01, 1.1843e-01, 1.8369e-01, 3.7368e-01,\n",
      "           9.1324e-01, 8.4936e-01, 6.9737e-01, 3.1418e-03, 9.2551e-01,\n",
      "           5.1879e-01, 7.1985e-01, 6.5435e-01, 1.4910e-01, 4.5592e-01,\n",
      "           4.0818e-01, 8.3347e-01, 2.9396e-01, 8.7069e-01, 5.6401e-01]]]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Double but got scalar type Float for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-f356e526db46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# outputs = model(A.unsqueeze(0).unsqueeze(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-162-a690f17ca406>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/myenv/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 340\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Double but got scalar type Float for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "# outputs = model(A.unsqueeze(0).unsqueeze(0))\n",
    "outputs = model(A.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
